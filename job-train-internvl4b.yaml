apiVersion: batch/v1
kind: Job
metadata:
  name: internvl4b-qlora-job
  namespace: nsf-maica
spec:
  backoffLimit: 1 # Default was 2 = It will try to restart (redo the job) 2 more times
  ttlSecondsAfterFinished: 86400
  template:
    spec:
      restartPolicy: Never

      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                  # - NVIDIA-GeForce-RTX-4090
                  # - Tesla-V100-SXM2-32GB
                  - NVIDIA-A10
      containers:
      - name: train
        image: docker.io/troyk500/internvl-nrp:v3
        imagePullPolicy: IfNotPresent
        terminationMessagePolicy: FallbackToLogsOnError
        command: ["/bin/bash", "-lc"]
        args:
          - |
            set -euxo pipefail
            python -V ; nvidia-smi || true

            echo "[start] Environment ready."
            echo "Cache and workspace initialized."

            # Model preload check
            if [ -d /workspace/models/InternVL4B ]; then
              echo "[preload] Model found in PVC; skipping download."
            else
              echo "[warning] Model directory not found in PVC."
            fi

            # Diagnostics
            ls -al /workspace/models/InternVL4B | head -n 20 || true
            ls -al /workspace/data || true

            # Start fine-tuning (environment handled in YAML env:)
            python /workspace/train_internvl_native.py

        env:
        - name: DISABLE_VERSION_CHECK
          value: "1"
        - name: HF_HOME
          value: /workspace/.cache/huggingface
        - name: TRANSFORMERS_CACHE
          value: /workspace/.cache/huggingface
        - name: HF_DATASETS_CACHE
          value: /workspace/.cache/huggingface/datasets
        - name: PIP_CACHE_DIR
          value: /workspace/.cache/pip
        - name: TMPDIR
          value: /workspace/tmp
        - name: HF_HUB_ENABLE_HF_TRANSFER
          value: "1"
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-token
              key: HUGGING_FACE_HUB_TOKEN
        - name: PYTORCH_CUDA_ALLOC_CONF
          value: "max_split_size_mb:64,expandable_segments:False"
        - name: TOKENIZERS_PARALLELISM
          value: "false"
        - name: OMP_NUM_THREADS
          value: "1"
        - name: MKL_NUM_THREADS
          value: "1"

        resources:
          requests:
            nvidia.com/gpu: 1   
            cpu: "12"
            memory: "28Gi"
            ephemeral-storage: "80Gi"
          limits:
            nvidia.com/gpu: 1    Running more than 1 GPU may cause parallelism
            cpu: "16"
            memory: "64Gi"
            ephemeral-storage: "100Gi"

        volumeMounts:
        - name: work
          mountPath: /workspace
        - name: cfg
          mountPath: /workspace/config

      volumes:
      - name: work
        persistentVolumeClaim:
          claimName: internvl-work
      - name: cfg
        configMap:
          name: internvl4b-traincfg
          items:
          - key: internvl4b_qlora_sft.yaml
            path: internvl4b_qlora_sft.yaml